{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency matrix (Example of lysine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adj_matrix(distance,site_kind):\n",
    "    # site_kind=\"K\"\n",
    "    path=f\"../data_process/{site_kind}_site/\"\n",
    "    print('Let\\'s start working on the distance matrix...')\n",
    "    name_seq = np.load(path+f'me_name_seq_{site_kind}_have_PDB_alphafold.npy',allow_pickle=True).item()\n",
    "    save_ori_path=f'../graph_information/{site_kind}_site'\n",
    "    save_path=save_ori_path+f'/Adj_matrix_{str(distance)}'\n",
    "    #创建文件夹\n",
    "    try:\n",
    "        os.makedirs(save_ori_path)\n",
    "    except FileExistsError:\n",
    "        print('FileExists')\n",
    "    #创建文件\n",
    "    try:\n",
    "        os.mkdir(save_path)\n",
    "    except FileExistsError:\n",
    "        print('File exists')\n",
    "    \n",
    "    for name in tqdm(list(name_seq.keys())):\n",
    "        if name+'.npy' not in os.listdir(save_path):\n",
    "            try:\n",
    "                posit_matrix=np.load(f\"../posit_matrix_new/{name}_F1.npy\") #your PDB save npy\n",
    "            except FileNotFoundError: \n",
    "                print(name)\n",
    "                continue\n",
    "            else:\n",
    "                length=posit_matrix.shape[0]\n",
    "                adj_matrix_bo=np.zeros([length,length])\n",
    "                for i in range(length):\n",
    "                    for j in range(i+1,length):\n",
    "                        if posit_matrix[i][j]<=distance:\n",
    "                            adj_matrix_bo[i][j]=1\n",
    "                            adj_matrix_bo[j][i]=1\n",
    "                np.save(save_path+'/'+name+'.npy',adj_matrix_bo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(adj, site, hop):\n",
    "    output = []\n",
    "    Q = []\n",
    "    Q.append(site)\n",
    "    for i in range(hop+1):\n",
    "        temp=[]\n",
    "        while Q != []:\n",
    "            v = Q.pop(0)\n",
    "            output.append(v)\n",
    "            for n ,t in enumerate(adj[v]):\n",
    "                if t==1 and n not in Q and n not in temp and n not in output:\n",
    "                    temp.append(n)\n",
    "        # print(temp)\n",
    "        Q=Q+temp\n",
    "        # print(Q)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extracted (One hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sentence):\n",
    "    word_index={'M': 0,\n",
    "    'I': 1,\n",
    "    'P': 2,\n",
    "    'L': 3,\n",
    "    'A': 4,\n",
    "    'C': 5,\n",
    "    'V': 6,\n",
    "    'G': 7,\n",
    "    'T': 8,\n",
    "    'Y': 9,\n",
    "    'D': 10,\n",
    "    'S': 11,\n",
    "    'Q': 12,\n",
    "    'W': 13,\n",
    "    'F': 14,\n",
    "    'K': 15,\n",
    "    'R': 16,\n",
    "    'E': 17,\n",
    "    'N': 18,\n",
    "    'H': 19,\n",
    "    'U': 20,\n",
    "    'X': 21}\n",
    "    sequence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sequence.append(word_index[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return sequence\n",
    "def onehot_feature(seq):\n",
    "    X_data=np.array(list(map(get_index, [seq])))\n",
    "    embedding_matrix_onehot=np.eye(22)\n",
    "    return embedding_matrix_onehot[X_data[0],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def positive_data(distance,hop,site_kind): \n",
    "    if site_kind=='K':\n",
    "        model_path=\"../feature_extract_model/lysine/\"\n",
    "    elif site_kind=='R':\n",
    "        model_path=\"../feature_extract_model/arginine/\"\n",
    "    w2v_model = Word2Vec.load(f\"{model_path}withX_word2vec_{site_kind}.model\")\n",
    "    embedding_matrix_word2vec = w2v_model.wv.vectors\n",
    "    vocab_list = list(w2v_model.wv.vocab.keys())\n",
    "    word_index = {word: index for index, word in enumerate(vocab_list)}\n",
    "    \n",
    "    count=0\n",
    "    path=\"../data_process/\"+site_kind+\"_site\"\n",
    "    name_seq=np.load(path+'/me_name_seq_'+site_kind+'_have_PDB_alphafold.npy',allow_pickle=True).item()\n",
    "    print('Start processing positive data')\n",
    "    positive_no=np.load(path+'/me_positive_name_site_'+site_kind+'_have_PDB_alphafold.npy',allow_pickle=True).item()\n",
    "    save_path='../embedding_data/'+str(hop)+'_hop_'+str(distance)+'_'+site_kind+'/positive'\n",
    "    try:\n",
    "        os.makedirs(save_path+'/adj')\n",
    "        os.makedirs(save_path+'/feat_onehot')\n",
    "        os.makedirs(save_path+'/feat_word2vec')\n",
    "        os.makedirs(save_path+'/seq')\n",
    "    except FileExistsError:\n",
    "        print('FileExists')\n",
    "\n",
    "    # positive_site=[]\n",
    "    # temp=np.load(f'/home/Users/gly/gly_me/data_process/{site_kind}_site/me_positive_name_site_{site_kind}_have_PDB_alphafold.npy',allow_pickle=True).item()\n",
    "    # for i,j in temp.items():\n",
    "    #     for k in range(len(j)):\n",
    "    #         positive_site.append(str(j[k]))\n",
    "    # # print(positive_site)\n",
    "\n",
    "    for name in tqdm(name_seq):\n",
    "        try:\n",
    "            adj_matrix=np.load(f'../graph_information/{site_kind}_site/Adj_matrix_{str(distance)}/{name}.npy')\n",
    "        except FileNotFoundError: \n",
    "            count+=1\n",
    "            print(name)\n",
    "            continue\n",
    "        else:\n",
    "            # feature_matrix=np.load('/home/chentb/alphafold_database/feature_matrix_1/'+name+'.npy')\n",
    "            sites=positive_no[name]\n",
    "            for site in sites:\n",
    "                try:\n",
    "                    num=bfs(adj_matrix,site-1,hop)\n",
    "                except IndexError:\n",
    "                    print(name,site)\n",
    "                seq=[]\n",
    "                word2vec_seq='X'+name_seq[name]+'X'\n",
    "                freture_ix_leaft=[]\n",
    "                freture_ix_right=[]\n",
    "                for i in num:\n",
    "                    # print(\"seq:\",seq)\n",
    "                    # print(\"name:\",name_seq[name][i])\n",
    "                    try:\n",
    "                        name_seq[name][i]\n",
    "                    except IndexError:\n",
    "                        print(name,i)\n",
    "                    seq.append(name_seq[name][i])\n",
    "                    ix_seq_leaft=word2vec_seq[i:i+2]\n",
    "                    ix_seq_right=word2vec_seq[i+1:i+3]\n",
    "                    freture_ix_leaft.append(word_index[ix_seq_leaft])\n",
    "                    freture_ix_right.append(word_index[ix_seq_right])\n",
    "                temp_feature_right=embedding_matrix_word2vec[freture_ix_right]\n",
    "                temp_feature_leaft=embedding_matrix_word2vec[freture_ix_leaft]\n",
    "                word2vec_feature=temp_feature_right+temp_feature_leaft\n",
    "\n",
    "                temp_adj_matrix=adj_matrix[num][:,num]\n",
    "\n",
    "                np.save(save_path+'/seq/'+name+'_'+str(site)+'.npy', num)\n",
    "                np.save(save_path+'/adj/'+name+'_'+str(site)+'.npy',temp_adj_matrix)\n",
    "                np.save(save_path+'/feat_onehot/'+name+'_'+str(site)+'.npy',onehot_feature(seq))\n",
    "                np.save(save_path+'/feat_word2vec/'+name+'_'+str(site)+'.npy',word2vec_feature)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_data(distance,hop,site_kind):\n",
    "    if site_kind=='K':\n",
    "        model_path=\"../feature_extract_model/lysine/\"\n",
    "    elif site_kind=='R':\n",
    "        model_path=\"../feature_extract_model/arginine/\"\n",
    "    w2v_model = Word2Vec.load(f\"{model_path}withX_word2vec_{site_kind}.model\")\n",
    "    embedding_matrix_word2vec = w2v_model.wv.vectors\n",
    "    vocab_list = list(w2v_model.wv.vocab.keys())\n",
    "    word_index = {word: index for index, word in enumerate(vocab_list)}\n",
    "    \n",
    "    count=0\n",
    "    path=\"../data_process/\"+site_kind+\"_site\"\n",
    "    name_seq=np.load(path+'/me_name_seq_'+site_kind+'_have_PDB_alphafold.npy',allow_pickle=True).item()\n",
    "    print('Start processing negative data')\n",
    "    negative_name_no=np.load(path+'/me_negative_name_site_'+site_kind+'_have_PDB_alphafold.npy',allow_pickle=True).item()\n",
    "    save_path='../embedding_data/'+str(hop)+'_hop_'+str(distance)+'_'+site_kind+'/negative'\n",
    "    try:\n",
    "        os.makedirs(save_path+'/adj')\n",
    "        os.makedirs(save_path+'/feat_onehot')\n",
    "        os.makedirs(save_path+'/feat_word2vec')\n",
    "        os.makedirs(save_path+'/seq')\n",
    "    except FileExistsError:\n",
    "        print('FileExistsError')\n",
    "    # negative_site=[]\n",
    "    # temp=np.load(f'../data_process/{site_kind}_site/me_negative_name_site_{site_kind}_have_PDB_alphafold.npy',allow_pickle=True).item()\n",
    "    # for i,j in temp.items():\n",
    "    #     for k in range(len(j)):\n",
    "    #         negative_site.append(str(j[k]))\n",
    "    # # print(negative_site)\n",
    "\n",
    "    \n",
    "    for name in tqdm(name_seq):\n",
    "        try:\n",
    "            adj_matrix=np.load(f'../graph_information/{site_kind}_site/Adj_matrix_{str(distance)}/{name}.npy')\n",
    "        except FileNotFoundError: \n",
    "            count+=1\n",
    "            continue\n",
    "        else:\n",
    "            sites=negative_name_no[name]\n",
    "            for site in sites:\n",
    "                try:\n",
    "                    num=bfs(adj_matrix,site-1,hop)\n",
    "                except IndexError:\n",
    "                    print(name,site)\n",
    "                seq=[]\n",
    "                word2vec_seq='X'+name_seq[name]+'X'\n",
    "                freture_ix_leaft=[]\n",
    "                freture_ix_right=[]\n",
    "                for i in num:\n",
    "                    seq.append(name_seq[name][i])\n",
    "                    ix_seq_leaft=word2vec_seq[i:i+2]\n",
    "                    ix_seq_right=word2vec_seq[i+1:i+3]\n",
    "                    freture_ix_leaft.append(word_index[ix_seq_leaft])\n",
    "                    freture_ix_right.append(word_index[ix_seq_right])\n",
    "                temp_feature_right=embedding_matrix_word2vec[freture_ix_right]\n",
    "                temp_feature_leaft=embedding_matrix_word2vec[freture_ix_leaft]\n",
    "                word2vec_feature=temp_feature_right+temp_feature_leaft\n",
    "\n",
    "                temp_adj_matrix=adj_matrix[num][:,num]\n",
    "\n",
    "                np.save(save_path+'/seq/'+name+'_'+str(site)+'.npy', num)\n",
    "                np.save(save_path+'/adj/'+name+'_'+str(site)+'.npy',temp_adj_matrix)\n",
    "                np.save(save_path+'/feat_onehot/'+name+'_'+str(site)+'.npy',onehot_feature(seq))\n",
    "                np.save(save_path+'/feat_word2vec/'+name+'_'+str(site)+'.npy',word2vec_feature)\n",
    "    print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's start working on the distance matrix...\n",
      "FileExists\n",
      "File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 742/742 [00:00<00:00, 3293.87it/s]\n"
     ]
    }
   ],
   "source": [
    "site_kind='K'\n",
    "Adj_matrix(10,site_kind=site_kind)\n",
    "positive_data(10,3,site_kind)\n",
    "negative_data(10,3,site_kind)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a11d0628d139d7e7c9149788f5917d633070c657f6dcf48713f9630f1a0a32ee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('GCN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a11d0628d139d7e7c9149788f5917d633070c657f6dcf48713f9630f1a0a32ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
